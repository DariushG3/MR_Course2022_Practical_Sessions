---
title: "Practical 1: Mendelian randomization using individual-level data (interactive)"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
    code_download: true
    code_folding: show
    css: style.css
    #number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Setup

## Install and load libraries
```{r eval=FALSE, echo=TRUE, code_folding="show"}
install.packages("ivpack")
library(ivpack)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(ivpack)
```

## Load the data
The 'coursedata.csv' file contains example data and is provided as part of the course materials. The file consists of fictitious (simulated) data on measurements for 1\thinspace000 participants of the risk factor (<tt>`x`</tt>), a continuous outcome (<tt>`y`</tt>), a binary outcome (<tt>`y.bin`</tt>), and four genetic variants (<tt>`g1`</tt>, <tt>`g2`</tt>, <tt>`g3`</tt>, <tt>`g4`</tt>). Set the working directory to where you have saved the 'coursedata.csv' file and then load the file into R or RStudio. 
\vspace{6pt}
```{r eval=TRUE}
rm(list=ls()) #Remove any existing objects in R 
#setwd("C:/Users/Amy/Documents/MR_practicals/MR-Course2") #Change with your location
coursedata = read.csv("C:\\Users\\dghasemisemeskandeh\\Desktop\\MR Course March 2022\\Practicals Data\\coursedata.csv") #Load data
attach(coursedata) #Attach coursedata to the R search path 
```
\vspace{48pt}

Use the <tt>`str`</tt> and <tt>`head`</tt> functions to look at the structure of the data.

###  {.tabset .tabset-fade .tabset-pills}
#### 
#### Code
```{r eval=FALSE}
str(coursedata) #Info on the structure of the data 
head(coursedata) #Show first 6 entries 
```
#### Answer
```{r eval =TRUE}
str(coursedata) #Info on the structure of the data 
head(coursedata) #Show first 6 entries 
```



# 1.	Causal estimate using the ratio method for a continuous outcome
The causal effect of the risk factor <tt>`x`</tt> on the continuous outcome <tt>`y`</tt> can be estimated as the per allele genetic association with the outcome $\hat{\beta}_{Y_{j}}$ divided by the per allele genetic association with the risk factor $\hat{\beta}_{X_{j}}$ (the ratio method):
$$ratio\ estimate\ for\ variant\ j = \frac{\hat{\beta}_{Y_{j}}}{\hat{\beta}_{X_{j}}}$$
\vspace{6pt}



### 

a). Use the code below to calculate the ratio causal estimate for the first genetic variant <tt>`g1`</tt>. If you get stuck with a command then use the `help` function, for example, type `help(lm)` into the command line.  
\vspace{6pt}
```{r, eval=TRUE}
by1 = lm(y~g1, data = coursedata)$coef[2] #Genetic association with the outcome
bx1 = lm(x~g1, data = coursedata)$coef[2] #Genetic association with the exposure
```
\vspace{48pt}

###  {.tabset .tabset-fade .tabset-pills}

####  

\vspace{48pt}

#### Code 
\vspace{6pt}
```{r, echo = TRUE, eval = FALSE}
beta.ratio1 = by1/bx1  
beta.ratio1 #Ratio estimate for g1 
```

\vspace{48pt}

#### Answer

```{r, echo = TRUE, eval = TRUE}
beta.ratio1 = by1/bx1  
beta.ratio1 #Ratio estimate for g1 
```

\vspace{48pt}

###

b). The standard error of the causal estimate can be calculated simply as the standard error of the genetic association with the outcome $se(\hat{\beta}_{Y_{j}})$ divided by the genetic association with the risk factor $\hat{\beta}_{X_{j}}$. This is the simplest form of the standard error, and is the first-order term from a delta method expansion for the standard error of a ratio.
$$standard\ error\ of\ ratio\ estimate\ (first\ order) = \frac{se(\hat{\beta}_{Y_{j}})}{\hat{\beta}_{X_{j}}}$$
\vspace{6pt}
Calculate the first order standard error of the ratio estimate for the first genetic variant <tt>`g1`</tt>. The following code for finding the standard error of the G-Y association will be helpful.
\vspace{6pt}
```{r, eval=TRUE}
byse1 = summary(lm(y~g1))$coef[2,2] #Standard error of the G-Y association
```
\vspace{48pt}

###  {.tabset .tabset-fade .tabset-pills}

####  

#### Code

\vspace{6pt}
```{r, eval=FALSE}
se.ratio1first = byse1/sqrt(bx1^2)  
se.ratio1first #Standard error (first order) of the ratio estimate 
```
\vspace{48pt}

#### Answer 

```{r, eval=TRUE}
se.ratio1first = byse1/sqrt(bx1^2)  
se.ratio1first #Standard error (first order) of the ratio estimate 
```
\vspace{48pt}

_Since the estimate of bx1 is not guaranteed to be positive, we take the square, and then the square root of bx1, when calculating the first order standard error of the ratio estimate.  The standard error will not make sense if bx1 is negative._

\vspace{12pt}

###
c). The above approximation does not account for the uncertainty in the denominator of the ratio estimate. This can be taken into account using the second term of the delta method expansion:
$$standard\ error\ of\ ratio\ estimate\ (second\ order) =  \sqrt{\frac{se(\hat{\beta}_{Y_{j}})^2}{{\hat{\beta}_{X_{j}}}^2} + \frac{{\hat{\beta}_{Y_{j}}}^2{se(\hat{\beta}_{X_{j}})}^2}{{\hat{\beta}_{X_{j}}}^4}} $$
\vspace{6pt}
Calculate the second order standard error of the ratio estimate for the first genetic variant <tt>`g1`</tt>.
\vspace{48pt}

###  {.tabset .tabset-fade .tabset-pills}
####
#### Code
\vspace{6pt}
```{r, eval=FALSE}
bxse1 = summary(lm(x~g1))$coef[2,2] #Standard error of the G-X association
se.ratio1second = sqrt(byse1^2/bx1^2 + by1^2*bxse1^2/bx1^4)
se.ratio1second #Standard error (second order) of the ratio estimate 
```
\vspace{48pt}
#### Answer 
```{r, eval=TRUE}
bxse1 = summary(lm(x~g1))$coef[2,2] #Standard error of the G-X association
se.ratio1second = sqrt(byse1^2/bx1^2 + by1^2*bxse1^2/bx1^4)
se.ratio1second #Standard error (second order) of the ratio estimate 
```

\vspace{48pt}
###

d). The F-statistic from the regression of the risk factor on the genetic variant(s) is used as a measure of 'weak instrument bias', with smaller values suggesting that the estimate may suffer from weak instrument bias. Calculate the F-statistic for the first genetic variant <tt>`g1`</tt>. [Hint: use `summary`]
\vspace{48pt}

###  {.tabset .tabset-fade .tabset-pills}
####
#### Code
\vspace{6pt}
```{r eval=FALSE}
fstat1 = summary(lm(x~g1))$f[1]
fstat1
```

\vspace{48pt}
#### Answer
```{r eval=TRUE}
fstat1 = summary(lm(x~g1))$f[1]
fstat1
```
\vspace{48pt}
###
_Care should be taken when interpreting the F-statistic from observed data. Some studies recommend excluding genetic variants if they have a F-statistic less than 10. Using such a stringent cut-off may introduce more bias than it prevents, as the estimated F-statistic can show considerable variation and may not provide a good indication of the true strength of the instrument._  

e). The Minor Allele Frequency (MAF) is the frequency at which the second most common allele occurs in a given population. Calculate the MAF for <tt>`g1`</tt>, remembering that some people may have 2 copies of the allele. 

\vspace{48pt}

###  {.tabset .tabset-fade .tabset-pills}
####
\vspace{48pt}
#### Code
```{r eval=FALSE}
MAF = (sum(g1==1) + 2*sum(g1==2))/(2*length(g1))
MAF
```
\vspace{48pt}
#### Answer
```{r eval=TRUE}
MAF = (sum(g1==1) + 2*sum(g1==2))/(2*length(g1))
MAF
```
\vspace{48pt}
###

f). We have, in the best Blue Peter tradition, calculated these values for the other 3 genetic variants. Load in this data from `summarized_data.csv` file  and compare to your results for <tt>`g1`</tt>. 

###  {.tabset .tabset-fade .tabset-pills}
#### Code
```{r, eval=FALSE}
ratio.all<-as.matrix(read.csv("summarized_data.csv", row=1)) 
ratio.all
```
\vspace{48pt}
#### Data Table
```{r echo=FALSE}
ratio.all<-as.matrix(read.csv("summarized_data.csv", row=1)) 
ratio.all
```
\vspace{48pt}
###
Using the estimates from ratio.all answer the following questions: 

* For the first order approximation, which genetic variant has the most precise ratio estimate (lowest standard error)? What influences the precision of the ratio estimate?

###  {.tabset .tabset-fade .tabset-pills}
#### 
#### Answer
_<tt>`g2`</tt> has the most precise ratio estimate according to the first order standard error. Variants with stronger associations with the risk factor will have smaller standard errors for the ratio estimate. Variants with low MAF will generally have large standard errors for the ratio estimate. _

\vspace{48pt}
###

* Do the standard errors for the ratio estimate differ for the first order and second order approximation? When do they differ the most?

###  {.tabset .tabset-fade .tabset-pills}
#### 
#### Answer
_There is little difference in the standard errors for the first and second order approximations for <tt>`g1`</tt> and <tt>`g2`</tt>. The second order approximations are noticeably larger than the first order approximations for <tt>`g3`</tt> and <tt>`g4`</tt>. The first and second order standard errors differ the most when the genetic association with the risk factor is imprecise._

\vspace{48pt}
###

g) Compute the observational association by regressing y on x. 

###  {.tabset .tabset-fade .tabset-pills}
#### 
#### Code
\vspace{6pt}
```{r, highlight=TRUE}
lm(y~x)$coef[2]
```

\vspace{48pt}
#### Answer
\vspace{6pt}
```{r, highlight=TRUE}
lm(y~x)$coef[2]
```

\vspace{48pt}
###
* How does it compare to the previous estimates?

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Answer
\vspace{6pt}
_The estimate for the observational association suggests there is an negative effect of the risk factor on the outcome. However the ratio estimates for <tt>`g2`</tt>, <tt>`g3`</tt> and <tt>`g4`</tt> indicated a positive effect._

\vspace{48pt}
###

\clearpage

# 2.	Causal estimate using the two-stage least squares method for a continuous outcome
The causal effect of the risk factor <tt>`x`</tt> on the continuous outcome <tt>`y`</tt> can also be estimated by the two-stage least squares (2SLS or TSLS) method. Two-stage least squares is performed by: 

* Regressing the risk factor on all the genetic variants in the same model, and storing the fitted values of the risk factor (R code: `fitted.values=lm(x~g1+g2+g3+g4)$fitted`). 
* A regression is then performed with the outcome on the fitted values of the risk factor (R code: `lm(y~fitted.values)`)
\vspace{12pt}

a). Perform the two-stage least squares method "by hand" (that is, by doing the two regression stages yourself). Make a note of the estimate and standard error of the second stage regression.
\vspace{48pt}

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Code

```{r eval=FALSE}
by.hand.fitted.values<-lm(x~g1+g2+g3+g4)$fitted
by.hand<-lm(y~by.hand.fitted.values)
summary(by.hand)$coef[2]  #estimate
summary(by.hand)$coef[2,2] # standard error
```
\vspace{48pt}
#### Answer
```{r eval=TRUE}
by.hand.fitted.values<-lm(x~g1+g2+g3+g4)$fitted
by.hand<-lm(y~by.hand.fitted.values)
summary(by.hand)$coef[2] # estimate
summary(by.hand)$coef[2,2] # standard error
```
\vspace{48pt}
###

b). Performing two-stage least squares by hand is generally discouraged, as the standard error in the second-stage of the regression does not take into account uncertainty in the first-stage regression. The R package \emph{ivpack} performs the two-stage least squares method using the `ivreg` function:
\vspace{12pt}
```{r, eval=TRUE}
ivmodel.all = ivreg(y~x|g1+g2+g3+g4, x=TRUE)
```
\vspace{6pt}
Compare the estimate and standard error from the calculation "by hand" and using the `ivreg` function. Is there a difference in the estimates from the two approaches?

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Code
```{r eval=FALSE}
summary(ivmodel.all)$coef[2] #2SLS estimate 
summary(ivmodel.all)$coef[2,2] #Standard error of the 2SLS estimate 
```
\vspace{48pt}
#### Answer
```{r}
summary(ivmodel.all)$coef[2] #2SLS estimate 
summary(ivmodel.all)$coef[2,2] #Standard error of the 2SLS estimate 
```
\vspace{6pt}
_The estimates are the same, but the standard error is slightly larger using the <tt>`ivreg`</tt> function as it takes into account the uncertainty in the first stage regression._
\vspace{48pt}

###

c). What is the F statistic for the model with all of the genetic variants?

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Code
```{r eval=FALSE}
summary(lm(x~g1+g2+g3+g4))$f[1] #f stat
```
\vspace{48pt}
#### Answer
```{r}
summary(lm(x~g1+g2+g3+g4))$f[1] #f stat
```
\vspace{48pt}
###


d). Perform the two-stage least squares method based only on the first genetic variant <tt>`g1`</tt>. 

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Code
```{r eval=FALSE}
ivmodel.g1 = ivreg(y~x|g1, x=TRUE)
summary(ivmodel.g1)$coef[2] #2SLS estimate for g1 only 
summary(ivmodel.g1)$coef[2,2] #Standard error of the 2SLS estimate for g1 only
```
\vspace{48pt}
#### Answer
\vspace{6pt}
```{r}
ivmodel.g1 = ivreg(y~x|g1, x=TRUE)
summary(ivmodel.g1)$coef[2] #2SLS estimate for g1 only 
summary(ivmodel.g1)$coef[2,2] #Standard error of the 2SLS estimate for g1 only
```
\vspace{48pt}
###

* How does the estimate and standard error compare to the ratio estimate and standard error for <tt>`g1`</tt> in Section 1?

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Answer
_The estimates from the two-stage least squares method and the ratio method are the same.  The standard errors from the two approaches are very similar._
\vspace{48pt}


\clearpage

# 3.	Causal estimate for a binary outcome
The causal effect of the risk factor <tt>`x`</tt> on the binary outcome <tt>`y.bin`</tt> can also be estimated as the per allele genetic association with the outcome divided by the per allele genetic association with the risk factor. However, with a binary outcome, logistic regression should be used for regression of the outcome on the genetic variant, particularly in a case-control setting. Also, in a case control setting, it is usual to regress the risk factor on the genetic variant in controls only. The `glm` function with an extra argument of `family = binomial` can be used to perform logistic regression in the same way that `lm` is used to perform linear regression.
\vspace{12pt}

a). Evaluate the ratio estimate and its standard error for <tt>`g1`</tt> using logistic regression (R code: `glm(y.bin~g1, family=binomial)`) to calculate the numerator (gene-outcome association), and linear regression in controls only (R code: `lm(x[y.bin==0]~g1[y.bin==0])`) to calculate the denominator.

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Code
```{r eval=FALSE}
by1.bin   = glm(y.bin~g1, family=binomial)$coef[2] #logistic regression for  
                                                   #gene-outcome association
byse1.bin = summary(glm(y.bin~g1, family=binomial))$coef[2,2]
bx1.bin   = lm(x[y.bin==0]~g1[y.bin ==0])$coef[2] #linear regression in the controls only
beta.ratio1.bin = by1.bin/bx1.bin
beta.ratio1.bin #ratio estimate for g1
se.ratio1.bin   = byse1.bin/bx1.bin
se.ratio1.bin #standard error of the ratio estimate for g1
```
\vspace{48pt}
#### Answer
```{r}
by1.bin   = glm(y.bin~g1, family=binomial)$coef[2] #logistic regression for  
                                                   #gene-outcome association
byse1.bin = summary(glm(y.bin~g1, family=binomial))$coef[2,2]
bx1.bin   = lm(x[y.bin==0]~g1[y.bin ==0])$coef[2] #linear regression in the controls only
beta.ratio1.bin = by1.bin/bx1.bin
beta.ratio1.bin #ratio estimate for g1
se.ratio1.bin   = byse1.bin/bx1.bin
se.ratio1.bin #standard error of the ratio estimate for g1
```
\vspace{48pt}
###


b). Calculate the two-stage least squares estimate for <tt>`g1`</tt> only. Recall we first regress the risk factor on the genetic variant - however in the binary case, we want to perform this regression on the controls only.
Then find fitted values for the model. Cases will still need a fitted value of the risk factor - this can be obtained by either using the <tt>`predict`</tt> function or by using the coefficients estimated in the first stage and calculating fitted values manually. 
Finally we regress the binary outcome on these fitted values using logistic regression.

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Code
```{r eval=FALSE}
g1.con = g1[y.bin ==0] #values for g1 in the controls only 
x.con  = x[y.bin ==0] #values for the risk factor in the controls only 
predict.con.g1 = predict(lm(x.con~g1.con), newdata=list(g1.con=g1)) #Generate predicted
  #values for all participants based on the linear regression in the controls only.  
tsls1.con = glm(y.bin~predict.con.g1, family=binomial) #Fit a logistic regression 
                                                #model on all the participants
summary(tsls1.con)$coef[2]
summary(tsls1.con)$coef[2,2] 
```
\vspace{48pt}
#### Answer
```{r}
g1.con = g1[y.bin ==0] #values for g1 in the controls only 
x.con  = x[y.bin ==0] #values for the risk factor in the controls only 
predict.con.g1 = predict(lm(x.con~g1.con), newdata=list(g1.con=g1)) #Generate predicted
  #values for all participants based on the linear regression in the controls only.  
tsls1.con = glm(y.bin~predict.con.g1, family=binomial) #Fit a logistic regression 
                                                #model on all the participants
summary(tsls1.con)$coef[2]
summary(tsls1.con)$coef[2,2] 
```
\vspace{48pt}
###


c). Repeat b). but calculate the two-stage least squares estimate for all of the genetic variants.

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Code
```{r eval=FALSE}
g2.con = g2[y.bin ==0] #values for g2 in the controls only 
g3.con = g3[y.bin ==0] #values for g3 in the controls only 
g4.con = g4[y.bin ==0] #values for g4 in the controls only 
predict.con<-predict(lm(x.con~g1.con+g2.con+g3.con+g4.con), #Predicted values 
        newdata=c(list(g1.con=g1),list(g2.con=g2),
                  list(g3.con=g3),list(g4.con=g4)))
tsls1.con.all = glm(y.bin~predict.con, family=binomial) #Logistic regression
summary(tsls1.con.all)$coef[2]
summary(tsls1.con.all)$coef[2,2] 
```
\vspace{48pt}
#### Answer
```{r}
g2.con = g2[y.bin ==0] #values for g2 in the controls only 
g3.con = g3[y.bin ==0] #values for g3 in the controls only 
g4.con = g4[y.bin ==0] #values for g4 in the controls only 
predict.con<-predict(lm(x.con~g1.con+g2.con+g3.con+g4.con), #Predicted values 
        newdata=c(list(g1.con=g1),list(g2.con=g2),
                  list(g3.con=g3),list(g4.con=g4)))
tsls1.con.all = glm(y.bin~predict.con, family=binomial) #Logistic regression
summary(tsls1.con.all)$coef[2]
summary(tsls1.con.all)$coef[2,2] 
```
\vspace{48pt}
###


d). What is the interpretation of the causal estimate in part c).?  

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Answer
_The estimate in d). represents the change in log causal odds ratio for `y.bin` per one unit increase in `x`._
\vspace{48pt}

###


e). Can you calculate a 95% confidence interval for the estimate in c). via a normal approximation?

###  {.tabset .tabset-fade .tabset-pills}
#### 
\vspace{48pt}
#### Answer
_Using a normal approximation (estimate $\pm 1.96\times$standard error), a 95% confidence interval for the log causal odds ratio in d). is (`r round(summary(tsls1.con.all)$coef[2] - 1.96*summary(tsls1.con.all)$coef[2,2],3)`, `r round(summary(tsls1.con.all)$coef[2] + 1.96*summary(tsls1.con.all)$coef[2,2],3)`). Taking the exponential of these values, we can obtain the 95% confidence interval for the odds ratio: (`r round(exp(summary(tsls1.con.all)$coef[2] - 1.96*summary(tsls1.con.all)$coef[2,2]),3)`, `r round(exp(summary(tsls1.con.all)$coef[2] + 1.96*summary(tsls1.con.all)$coef[2,2]),3)`)._
\vspace{48pt}

###

Finally clean up your workspace

```{r include=TRUE}
detach(coursedata)
rm(list=ls())
```
\clearpage
